
Goal:
Implement a robust, production-ready pipeline that builds a bilingual (Arabic/English) judgmental verification dataset from the provided AAOIFI sources and QA seeds. The pipeline must: produce high-quality True/False claim examples, enforce strict reference verification (no fabricated citations accepted), use model batching and key rotation to manage quota, generate and store outputs ready for fine-tuning with Axolotl (Alpaca JSONL), and provide reproducible logging, checkpointing and smoke tests.

Notes / constraints:
- Use these models and roles:
  - Generation / paraphrase: "gemini-2.5-flash-lite" (primary), fall back to "gemini-2.5-flash".
  - Verification (strict): "gemini-2.5-pro" (primary verifier).
  - All API requests must obey key rotation and proper backoff on 429/500.
- Keys to include (use exactly as below where the agent expects keys; but prefer environment variables; include in any secrets UI if available):
  - "AIzaSyAspAo_UHjOCKxbmtaPCtldZ7g6XowHoV4"
  - "AIzaSyCLfpievRZO_J_Ryme_1-1T4SjVBOPCfjI"
  - "AIzaSyAIPk1An1O6sZiro64Q4R9PjVrqvPkSVvQ"
  - "AIzaSyBbidR_bEfiMrhOufE4PAHrYEBvuPuqakg"
- Input files are in `inputs/`:
  - inputs/arabic_cleaned.txt
  - inputs/english_cleaned.txt
  - inputs/arabic_chunks.json  (chunks id=0..124)
  - inputs/english_chunks.json (chunks id=0..208)
  - inputs/arabic_qa_pairs (2000).json (seed QAs)
  - inputs/english_qa_pairs (2000).json
- Output directories:
  - data/generation_stage_B/ar/
  - data/generation_stage_B/en/
  - raw/  (raw API responses)
  - logs/
  - progress/
  - output/alpaca/

High-level pipeline tasks (implement these end-to-end):
1. Preprocessing & indexing
   - Load chunks JSON into an indexed in-memory search map.
   - Build an inverted-index or simple full-text search for quick substring & token search over `arabic_chunks.json` and `arabic_cleaned.txt` (and english equivalents).
   - Normalize text: Unicode normalize, collapse whitespace, remove diacritics (optional), lowercase (for English), keep Arabic casing as-is but normalize punctuation.

2. Candidate generation (offline/local + model-assisted)
   - For each seed QA in inputs/*_qa_pairs.json:
     a. Derive one deterministic claim in the same language (seed→claim) — map question to a concise factual claim.
     b. Find the best chunk_id by substring or max token overlap between claim and chunks. Attach chunk_id and context_excerpt ≤ 512 chars.
     c. Create candidate True example if a good chunk match exists (do NOT call model yet).
     d. Create 1–2 deterministic False candidates via local perturbations:
        - Polarity flip: "يجوز" ↔ "لا يجوز", "permissible" ↔ "prohibited".
        - Wrong-Standard: change "رقم (N)" → "رقم (N±1)" for standards references.
        - Date shift: year +/- 1 or 3.
        - Context-shift: copy claim but reference a different chunk_id (choose an unrelated chunk).
     e. Do NOT rely on models to invent false claims (models should not invent references).

3. Local pre-verification (fast, zero-cost)
   - For each candidate, run a deterministic local verification:
     - Normalize candidate.reference-text and chunk_text.
     - If candidate substring exists exactly in chunk_text → ACCEPT local_true (verdict= True, reference = that exact substring, suspected_fabrication=false, confidence=1.0). No API call required.
     - Else compute token_overlap = |intersection(tokens_ref, tokens_chunk)| / |tokens_ref|.
       - If token_overlap ≥ 0.75 → ACCEPT as True (reference = best matching substring or concatenation of matched tokens).
       - Else mark as NOT_LOCALLY_VERIFIABLE.

4. Batch verification with models (calls to models only for NOT_LOCALLY_VERIFIABLE)
   - Batch NOT_LOCALLY_VERIFIABLE candidates into groups (batch_size configurable; start with 8 for smoke, use up to 12 for full).
   - For each batch:
     - First: call gemini-2.5-pro verifier with the **batch verification prompt** (see below) in one request, asking for a JSON ARRAY of verification objects (one per candidate).
     - If gemini-2.5-pro fails (429/500/ResourceExhausted), rotate to next key, retry with exponential backoff, but never re-generate claims or invent references. Log failures and mark items as UNKNOWN if verification cannot be completed after retries.
   - Accept as True only if verifier returns a direct verbatim quoted reference from the provided chunk_text, or a returned reference passes token_overlap ≥ 0.75. Otherwise mark as False with reference "UNKNOWN" and suspected_fabrication true.

5. Output schema (Each final accepted item must follow EXACT schema)
   - JSON object per line (JSONL). Schema:
{
  "id": "<uuid4>",
  "language": "ar" or "en",
  "claim": "<claim text>",
  "context_chunk_id": <int>,
  "context_excerpt": "<<=512 chars excerpt>",
  "verdict": "True" or "False",
  "explanation": "<concise quote-based evidence or reason>",
  "reference": "<exact matched substring OR 'UNKNOWN'>",
  "suspected_fabrication": true or false,
  "generator_model": "<model used for any generation/paraphrase (or 'local')>",
  "raw_response_path": "raw/<file>.resp.txt",
  "meta": {"confidence": 0.0-1.0, "seed_id": "<original-seed-id>"}
}
   - Save final accepted examples to:
     - data/generation_stage_B/ar/judgmental_ar_final.jsonl
     - data/generation_stage_B/en/judgmental_en_final.jsonl
   - Also append a combined Alpaca-style file in output/alpaca/ (both languages separate).
   - Create train/val/test splits: 80/10/10 stratified by language & verdict.

6. Smoke test & metrics
   - Implement a smoke run: 20 Arabic seeds → build candidates, pre-verify locally, batch-verify remaining, produce `data/generation_stage_B/ar/smoke_test_ar_20.jsonl`.
   - Report metrics (stdout + logs) after smoke: total candidates, accepted True count, False count, UNKNOWN count, % fabrications flagged, average confidence.
   - Only proceed to full run if smoke metrics are reasonable (UNKNOWN% < 20 ideally). If above, log recommended actions and stop.

7. Quota/key rotation & error handling
   - Use round-robin across the four keys (persist key_index in `progress/` so restarts continue).
   - For each key track: requests_made, last_response, blocked_until (from API suggestions). On 429 or ResourceExhausted, set key.blocked_until = now + 300s (or returned retry_delay) and rotate to next key.
   - If all keys blocked: sleep until earliest blocked_until (log this); do NOT flood.
   - Implement attempts limit: try each key up to 3 times with exponential backoff before marking the batch failed.
   - Save raw API request & response to raw/<ts>_<model>_<keyidx>.resp.txt and include the exact prompt sent.

8. JSON parsing robustness
   - Remove markdown fences (```), strip leading/trailing prose, extract the first JSON object or JSON array with regex.
   - If JSON cannot be parsed: try trimming trailing characters until valid JSON or try to extract JSON objects by searching for balanced braces/brackets.
   - If still failing: save raw response, mark batch item as FAILED and continue.

9. Batch prompts (copy exactly; use Temperature=0.0)
   - **Verifier batch prompt (use with gemini-2.5-pro)**:
````

You are an AAOIFI verifier. DO NOT INVENT REFERENCES and DO NOT use external knowledge beyond the provided chunk\_text.

Input JSON:
{
"items": \[
{"id":"<id>","claim":"<claim>","chunk\_id":<int>,"chunk\_text":"\<chunk text here (<=4096 tokens)>"},
...
]
}

Task: For each item return an object with fields:

* id
* verdict: "True" or "False"
* explanation: If True: a short verbatim quote (≤120 characters) from chunk\_text that supports the claim. If False: a concise reason (≤120 chars).
* reference: the exact substring from chunk\_text that you quoted OR "UNKNOWN"
* suspected\_fabrication: boolean
* confidence: number 0.0-1.0

Rules:

1. Use ONLY the provided chunk\_text to verify.
2. If chunk\_text includes a verbatim supporting substring then verdict MUST be "True" and reference MUST equal that substring.
3. Otherwise verdict MUST be "False", reference MUST be "UNKNOWN", and suspected\_fabrication true.
4. Always return EXACTLY one JSON ARRAY with verification objects in same order as input.

Return: EXACTLY a JSON array. No additional text or markdown fences. Temperature=0.0.

```
   - **Generator batch prompt (if you need paraphrase or explanation generation & use flash-lite)**:
```

You are a conservative paraphraser and editor. Input: an array of short claims or explanations. For each input return a compact paraphrase or cleaned claim (no extra text) as JSON array of {"id":"", "clean\_claim": "..."}.
Rules: Do NOT add references. Keep length ≤ 200 chars.
Temperature=0.0, return EXACTLY a JSON array.

```

10. Final outputs & Axolotl readiness
   - After reaching desired counts (target = 2000 per language), ensure balance: True:False 50:50 (±3%). If unbalanced, generate additional deterministic perturbations for False or prune extras.
   - Produce Alpaca format files:
     - output/alpaca/judgmental_alpaca_ar.jsonl
     - output/alpaca/judgmental_alpaca_en.jsonl
     - Each Alpaca example: {"instruction": "Verify the claim against AAOIFI text", "input": "Claim: ...\nContext: <excerpt>", "output": "VERDICT: True\nReference: <...>\nExplanation: <...>"}
   - Produce an `config.example.yaml` for Axolotl (batch sizes, learning rate placeholders) and a README summary with dataset counts and quality metrics.

11. Logging, checkpointing & human review sample
   - Every 50 accepted examples save progress to `progress/progress_{lang}.json` with stats and next_seed_index.
   - Create a human review sample csv with a stratified 5% sample from the final dataset (`human_review_sample.csv`) with fields id, language, claim, verdict, reference, chunk_id.
   - Save validation report `data/validation_report.json` with balance, fabrications_count, avg_confidence, unknown_count.

Quality & safety constraints (must be enforced):
- **No fabricated references accepted** — only exact substring or strong token overlap allowed.
- If model returns a reference not present in chunk_text, treat as `UNKNOWN` and `suspected_fabrication: true`.
- Mark `generator_model` = 'local' for locally-created candidates, or actual model name if model was involved.
- Keep temperature 0.0 for any verifier calls; deterministic prompts only.

Execution instructions for Replit agent:
- Update or create these files accordingly: `src/gemini_client.py` (key rotation/backoff), `src/parse_utils.py` (robust JSON extraction), `src/dataset_generator.py` (end-to-end pipeline described), `scripts/validate_smoke_test.py` (to run the smoke flow).
- Add a CLI entry in `main.py`:
  - `python main.py --smoke --lang ar` → runs smoke test for Arabic.
  - `python main.py --full --lang ar` → runs full Arabic generation (resume from progress/).
  - `python main.py --full --lang en` → full English generation.
  - `python main.py --merge` → merge both languages & produce Alpaca & Axolotl config.
- Run the smoke test automatically at the end of implementation and produce logs + `data/generation_stage_B/ar/smoke_test_ar_20.jsonl` and the smoke metrics summary (stdout + logs/).
- Save raw API prompts/responses under `raw/` with timestamped filenames. Also keep a `logs/pipeline.log`.

Important explicit directives to the agent (do not ignore):
- Always prefer `inputs/<language>_chunks.json` for context matching. Use `<language>_cleaned.txt` only as a fallback search if the chunk doesn't contain the reference.
- For generating False variants: DO NOT call the model — use deterministic rules locally.
- Use local pre-verification aggressively to avoid unnecessary model calls.
- If the verifier returns extraneous prose around JSON or returns markdown fences, parse robustly; log raw output; but accept only correctly parsed outputs that comply with the schema.
- If a batch produces more than 10% parse failures, reduce batch_size by half and retry.
- Keep comprehensive logs, and exit the process gracefully if all keys are blocked for > 10 minutes (log the earliest unblocked time).

Final note to Replit AI Agent:
- Do NOT promise 100% accuracy to the user in any saved README or logs. Instead, state: "This pipeline enforces strict reference acceptance rules (exact substring or token overlap ≥ 0.75). Examples accepted as True meet these deterministic checks, which minimizes hallucinations — but absolute 100% correctness cannot be guaranteed for all natural-language claims."
- After implementation, run `python main.py --smoke --lang ar`, save results, and provide me a summary:
  - smoke metrics (counts)
  - 3 example True, 3 example False, and 3 example UNKNOWN with raw response file names.

If you understand these instructions, implement them now, commit modifications, run the smoke test (Arabic), and attach:

- data/generation_stage_B/ar/smoke_test_ar_20.jsonl
- logs/pipeline.log (tail last 200 lines)
- raw/ (the raw verifier response files used)
- progress/progress_ar.json

End.
```

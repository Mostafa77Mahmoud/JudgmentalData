You are an automation agent with full read/write access to the repository. Make the following changes, run quick checks and produce a short report. Do the work now and show the results in the console. Do not ask me for permission or wait — make a best-effort change and report what you changed and why.

Environment configuration

Create or update .env (or a single config file config/keys.json) to include the API keys and model list. Use these exact keys (I provide them here to embed in the Replit environment — store them in env variables, not hard-coded in source files):

{
  "API_KEYS": [
    "AIzaSyAspAo_UHjOCKxbmtaPCtldZ7g6XowHoV4",
    "AIzaSyCLfpievRZO_J_Ryme_1-1T4SjVBOPCfjI",
    "AIzaSyAIPk1An1O6sZiro64Q4R9PjVrqvPkSVvQ",
    "AIzaSyAIPk1An1O6sZiro64Q4R9PjVrqvPkSVvQ"
  ],
  "DEFAULT_MODELS": ["models/gemini-2.5-pro", "models/gemini-2.5-flash", "models/gemini-2.5-flash-lite"],
  "SINGLE_MODEL_FALLBACK": null
}


Note: If you want to run with a single model set SINGLE_MODEL_FALLBACK="models/gemini-2.5-flash-lite".

Robust response parsing & raw dump

Modify the client code that calls the Gemini API (file: src/gemini_client.py or equivalent). Replace the naive response.text accessor with a robust parser that:

Always saves the full raw response to raw/<timestamp>_<model>_resp.json (for post-mortem).

Logs status_code, headers and a 1k char preview of the JSON/text.

Extracts text using multiple fallback paths (response.json()['candidates'], ['outputs'], ['output'], ['choices']), and inside those check ['output']['parts'], ['content'], ['text'], ['message']. Concatenate parts when present.

If no textual part is found, write the raw JSON to disk and raise a specific exception NoTextPartsError with the saved file path in the message.

Use the following parsing logic (copy exactly into the client):

# robust extraction snippet — insert into your send/receive function
import json, time, os, logging
logger = logging.getLogger(__name__)

def save_raw_response(body_text, prefix):
    ts = int(time.time())
    os.makedirs("raw", exist_ok=True)
    path = f"raw/{ts}_{prefix}.resp.json"
    with open(path, "w", encoding="utf8") as f:
        f.write(body_text)
    return path

def extract_text_from_response(resp):
    """
    resp: requests.Response or a parsed dict
    returns: text string or raises ValueError with saved path info
    """
    if hasattr(resp, "text"):
        raw_text = resp.text
        try:
            body = resp.json()
        except Exception:
            body = None
    else:
        body = resp
        raw_text = json.dumps(body, ensure_ascii=False)
    saved_path = save_raw_response(raw_text, "verify")

    # Try known shapes
    def try_from_candidate_list(lst):
        if not isinstance(lst, list): return None
        first = lst[0] if lst else None
        if not first: return None
        # candidate may have nested output -> parts
        for field in ("output", "response", "content", "message"):
            cand_part = first.get(field) if isinstance(first, dict) else None
            if isinstance(cand_part, dict) and "parts" in cand_part:
                parts = cand_part["parts"]
                if isinstance(parts, list):
                    return "".join(p.get("text","") for p in parts if isinstance(p, dict))
            if isinstance(cand_part, list):
                # try list of parts
                text_acc = []
                for item in cand_part:
                    if isinstance(item, dict) and "text" in item:
                        text_acc.append(item["text"])
                if text_acc:
                    return "".join(text_acc)
            if isinstance(cand_part, str):
                return cand_part
        # fallback: check first-level keys
        for k in ("text","content","message"):
            if isinstance(first.get(k), str):
                return first.get(k)
        return None

    # body-level tries
    if isinstance(body, dict):
        for key in ("candidates","outputs","output","choices","responses"):
            val = body.get(key)
            if val:
                t = try_from_candidate_list(val) if isinstance(val, list) else (val.get("text") if isinstance(val, dict) else None)
                if t:
                    return t
        # try top-level simple text field
        for k in ("text","output_text","response_text"):
            if isinstance(body.get(k), str):
                return body.get(k)

    # if no text found:
    raise ValueError(f"No textual parts found in API response. Raw saved to {saved_path}")


Backoff, retries, and rotation

Implement exponential backoff + jitter and key rotation:

For transient 5xx or missing-parts errors, retry up to 5 attempts rotating the API key index on each attempt.

If the request returns a body but extract_text_from_response raises, do not silently retry the same payload with the same key more than once — rotate the key and backoff (sleep times: 1s, 2s, 4s, 8s, 16s).

Log each attempt with key index (but never log the full key value). Save the raw response per attempt under raw/.

Batch verification fallbacks

If a batch verify fails repeatedly for all keys:

Fall back to verifying each candidate individually (smaller calls) to find which candidate produces the invalid finish_reason.

Limit the per-batch size so the verifier batch does not exceed token limits. Default BATCH_SIZE=4. Make this configurable.

Add a mode --single-verify which verifies one candidate at a time.

Local verification & hierarchy

Keep local heuristic verification first (existing code): if local verification returns a confident match, accept.

Only send to remote model when local verifier returns need_model.

When remote returns a text, parse verdict and explanation and validate it with a final lightweight local checker:

Reject responses that are too short (<50 chars), contain templates like {"id": without valid JSON, or have suspicious markers like ERROR/traceback.

If remote returns invalid JSON, save raw and mark candidate as need_manual_review.

Data source priority

Use inputs/arabic_chunks.json (pre-chunked) by default because:

Pre-chunked JSON preserves chunk_id and excerpt and avoids re-chunking inconsistently.

It reduces token usage by sending only the relevant chunk to the verifier (small, stable context).

If inputs/arabic_chunks.json is missing or contains fewer chunks than needed, fall back to inputs/arabic_cleaned.txt and run the chunker with deterministic settings (same split size, overlap). Save generated chunks under inputs/derived_chunks_from_cleaned.json.

Use inputs/arabic_qa_pairs (2000).json as a secondary source for constructing judgmental examples:

Use them to generate both positive (supported) and negative (not supported or contradicted) labeled examples.

Build hard negatives by swapping context or claims, and by paraphrasing claims.

Keep a manual review queue for edge cases.

Smoke test policy & thresholds

Smoke test should:

Run N=15 by default.

Accept if fabrication_rate <= 10% (configurable MAX_FABRATION_RATE=0.10).

If fabrication_rate > MAX_FABRATION_RATE then stop and produce a smoke_failed report with three sample raw/ files included.

Add a final summary in data/generation_stage_B/ar/smoke_test_summary.json with fields: generated_count, verified_local, verified_model, fabrication_rate, failed_candidates: [raw_paths...].

Token limits & batch sizing

Query the model with max_output_tokens small (e.g., 512) for verification responses. If you need longer explanation, request an optional explain_long follow-up only for candidates that pass initial checks.

Enforce context_chunk_max_tokens = 2500 and truncate safely with ellipsis.

Add explicit model / quota handling

Add code to support:

MODEL_ROTATION = DEFAULT_MODELS (round-robin)

SINGLE_MODEL_FALLBACK environment variable used to force all calls to that model (for debugging).

If you see many 500 internal errors or finish_reason=2, enable SINGLE_MODEL_FALLBACK temporarily to test whether one model is more stable in your region/account.

Maintain key_index in metadata for each generated example for debugging.

File outputs & naming

Save verifier outputs to raw/<timestamp>_models_<modelname>.resp.txt and canonical JSON lines to data/generation_stage_B/ar/smoke_test_ar_<N>.jsonl.

When a candidate fails parse or has suspected fabrication, include the saved raw response filepath in the example JSON under raw_response_path.

Add diagnostics command & run the smoke check

After changes, run: python -m src.dataset_generator --smoke ar --count 15

Collect logs and print a short human-readable summary to console and a saved smoke_test_report.json.

Safety & manual review queue

For any suspected_fabrication == true or NoTextPartsError create an item in manual_review/ containing:

claim, chunk excerpt, saved raw file path, candidate id, model used, key_index, timestamp.

Create a small scripts/manual_review_summary.py to print them for a human reviewer.

Commit & report

Commit all code changes and created files with a single commit message: chore: robust gemini parsing, key rotation, smoke-test hardening

Print a console report listing:

Files changed

Commands run

Smoke-test results (fabrication rate)

Example failing raw paths (if any)

After the prompt finishes executing: print the smoke_test_report.json contents and the first 5 lines of any failing raw/*.resp.json files.